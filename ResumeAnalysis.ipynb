{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EbECtZlCmP8r0ySHR-Nnd9cECMMzTaFq",
      "authorship_tag": "ABX9TyMqX2SFP6t/4cxtmIeoe37U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hent0mi/resume-scorer-colab-analysis/blob/main/ResumeAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZgVVTbVLjYM",
        "outputId": "d9983e34-7750-4350-97fa-ecc88d0b69bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# Optional: Verify the mount by listing your app folder\n",
        "# !ls /content/drive/MyDrive/ResumeScorerApp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk spacy PyPDF2 python-docx google-generativeai\n",
        "!python -m spacy download en_core_web_sm # Download a small English model for spaCy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFc8xzovNprh",
        "outputId": "2e3b0f3c-012d-44ac-add2-afd648238d8b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re # Import the regular expression module\n",
        "import os\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import spacy\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "\n",
        "# --- Existing setup from previous steps ---\n",
        "# Load a small English language model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Configure Gemini API (assuming GEMINI_API_KEY is set in Colab Secrets)\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY secret not found or not enabled for this notebook.\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Define a broad list of common IT skills\n",
        "common_skills = [\n",
        "    # Core Programming & Data\n",
        "    \"python\", \"java\", \"javascript\", \"c++\", \"c#\", \"go\", \"ruby\", \"php\", \"swift\", \"kotlin\",\n",
        "    \"sql\", \"nosql\", \"mongodb\", \"postgresql\", \"mysql\", \"redis\",\n",
        "    \"data analysis\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\",\n",
        "\n",
        "    # Cloud & DevOps\n",
        "    \"aws\", \"azure\", \"google cloud\", \"docker\", \"kubernetes\", \"terraform\",\n",
        "    \"git\", \"jira\", \"agile\", \"scrum\", \"devops\", \"ci/cd\",\n",
        "\n",
        "    # General Business & Soft Skills\n",
        "    \"project management\", \"leadership\", \"communication\", \"teamwork\", \"problem-solving\",\n",
        "    \"microsoft office\", \"excel\", \"word\", \"powerpoint\", \"outlook\",\n",
        "    \"customer service\",\n",
        "\n",
        "    # Web Development\n",
        "    \"react\", \"angular\", \"vue.js\", \"node.js\", \"django\", \"flask\", \"spring\", \"laravel\",\n",
        "    \"web development\", \"frontend\", \"backend\", \"fullstack\", \"api development\", \"restful apis\",\n",
        "    \"ui/ux design\", \"figma\", \"sketch\", \"adobe xd\",\n",
        "\n",
        "    # Generative AI Specific Skills\n",
        "    \"generative ai\", \"genai\", \"artificial intelligence\", \"large language models\", \"llms\", \"ai models\",\n",
        "    \"prompt engineering\", \"prompting\", \"ai ethics\", \"model evaluation\", \"fine-tuning\",\n",
        "    \"retrieval augmented generation\", \"rag\", \"embeddings\", \"vector databases\",\n",
        "\n",
        "    # Process Optimization & Automation\n",
        "    \"process optimization\", \"business process automation\", \"bpa\", \"workflow automation\",\n",
        "    \"process analysis\", \"lean six sigma\", \"efficiency improvement\", \"automation\",\n",
        "    \"robotics process automation\", \"rpa\", \"business analysis\",\n",
        "\n",
        "    # Low-Code/No-Code Platforms\n",
        "    \"low-code\", \"no-code\", \"power platform\", \"microsoft power automate\", \"zapier\", \"make.com\",\n",
        "    \"bubble.io\", \"webflow\", \"appian\", \"outsystems\", \"mendix\", \"salesforce flow\",\n",
        "\n",
        "    # Analytical & Troubleshooting\n",
        "    \"troubleshooting\", \"root cause analysis\", \"diagnostic skills\", \"analytical thinking\",\n",
        "    \"data interpretation\", \"critical thinking\", \"problem diagnosis\",\n",
        "\n",
        "    # AI/ML Fundamentals (Expanded from generic ML)\n",
        "    \"machine learning\", \"deep learning\", \"nlp\", \"natural language processing\", \"computer vision\",\n",
        "    \"tensorflow\", \"pytorch\", \"scikit-learn\", \"data science\", \"model deployment\",\n",
        "    \"model monitoring\", \"feature engineering\", \"statistical analysis\",\n",
        "\n",
        "    # Communication & Training (for the analyst role)\n",
        "    \"training\", \"user training\", \"documentation\", \"technical writing\", \"presentations\",\n",
        "    \"stakeholder management\", \"user adoption\", \"change management\"\n",
        "]\n",
        "\n",
        "# Define base paths for Google Drive folders\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/ResumeScorerApp/'\n",
        "JD_FOLDER = os.path.join(DRIVE_BASE_PATH, 'JobDescriptions')\n",
        "RESUMES_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Resumes')\n",
        "RESULTS_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Results')\n",
        "RESULTS_FILE_NAME = 'analysis_results.json' # Fixed name for easy retrieval by n8n\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "# File Reading Functions: Create functions to read content from PDF, DOCX, and TXT files (read_text_from_pdf, read_text_from_docx, read_file_content).\n",
        "def read_text_from_pdf(filepath):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_text_from_docx(filepath):\n",
        "    \"\"\"Extracts text from a DOCX file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = Document(filepath)\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            full_text.append(para.text)\n",
        "        text = '\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_file_content(filepath):\n",
        "    \"\"\"Determines file type and calls appropriate reader function.\"\"\"\n",
        "    if filepath.endswith('.pdf'):\n",
        "        return read_text_from_pdf(filepath)\n",
        "    elif filepath.endswith('.docx'):\n",
        "        return read_text_from_docx(filepath)\n",
        "    elif filepath.endswith('.txt'):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading TXT {filepath}: {e}\")\n",
        "            return \"\"\n",
        "    return \"\" # Return empty string for unsupported types\n",
        "\n",
        "# Skill Extraction: spaCy will be used for basic keyword matching against a comprehensive list of skills. (extract_skills)\n",
        "def extract_skills(text, skill_keywords_list):\n",
        "    \"\"\"Extracts skills from text based on a predefined keyword list.\"\"\"\n",
        "    if nlp is None: # Check if nlp model loaded successfully\n",
        "        print(\"Warning: spaCy NLP model not loaded. Skill extraction will be limited.\")\n",
        "        return [] # Return empty list if NLP is not available\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "    found_skills = set()\n",
        "    for skill in skill_keywords_list:\n",
        "        if skill.lower() in doc.text:\n",
        "            found_skills.add(skill)\n",
        "    return list(found_skills)\n",
        "\n",
        "#  Scoring Algorithm: A basic Jaccard similarity based on shared skills. (calculate_score)\n",
        "def calculate_score(jd_skills, resume_skills):\n",
        "    \"\"\"Calculates a compatibility score based on shared skills.\"\"\"\n",
        "    if not jd_skills or not resume_skills:\n",
        "        return 0.0 # No skills to compare\n",
        "    jd_set = set(jd_skills)\n",
        "    resume_set = set(resume_skills)\n",
        "    intersection = len(jd_set.intersection(resume_set))\n",
        "    union = len(jd_set.union(resume_set))\n",
        "    return (intersection / union) * 100 if union > 0 else 0.0 # Score out of 100\n",
        "\n",
        "# Resume Summarization: Use the Gemini API to generate concise summaries.(summarize_resume)\n",
        "def summarize_resume(resume_text, job_description_text):\n",
        "    \"\"\"Generates a concise summary of a resume relevant to a job description using Gemini.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Use gemini-1.5-flash for speed and cost-effectiveness\n",
        "    prompt = f\"\"\"Summarize the following resume in 2-3 concise sentences, focusing on skills, experience, and qualifications that are most relevant to the provided job description.\n",
        "\n",
        "    Job Description:\n",
        "    {job_description_text}\n",
        "\n",
        "    Resume:\n",
        "    {resume_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Calling synchronous generate_content ---\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing resume with Gemini: {e}\")\n",
        "        return \"Summary unavailable due to processing error.\"\n",
        "\n",
        "# Extract Contact Information: Extracting personal information like name, and email (extract_contact_info)\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"\n",
        "    Extracts candidate name, and email from resume text.\n",
        "    This is a heuristic approach and may not be 100% accurate.\n",
        "    \"\"\"\n",
        "    name = \"N/A\"\n",
        "    email = \"N/A\"\n",
        "\n",
        "    # 1. Extract Email (most reliable with regex)\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    if emails:\n",
        "        email = emails[0] # Take the first email found\n",
        "\n",
        "    # 2. Extract Name (very challenging without advanced NLP)\n",
        "    # This is a very basic heuristic: often the first few lines of a resume contain the name.\n",
        "    # We'll try to find a capitalized phrase at the beginning.\n",
        "    lines = text.strip().split('\\n')\n",
        "    if lines:\n",
        "        first_line = lines[0].strip()\n",
        "        # Look for a line that seems like a name (e.g., \"John Doe\" or \"J. Doe\")\n",
        "        # This regex looks for 2-4 capitalized words, potentially with periods.\n",
        "        name_pattern = r'^[A-Z][a-z]+(?: [A-Z][a-z]+){1,3}$|^[A-Z]\\. [A-Z][a-z]+(?: [A-Z][a-z]+)?$'\n",
        "        if re.match(name_pattern, first_line):\n",
        "            name = first_line\n",
        "        else:\n",
        "            # Fallback: try to find a capitalized phrase in the first few lines\n",
        "            for i in range(min(len(lines), 3)): # Check first 3 lines\n",
        "                line = lines[i].strip()\n",
        "                # Simple check for multiple capitalized words\n",
        "                potential_name_parts = [word for word in line.split() if word and word[0].isupper()]\n",
        "                if len(potential_name_parts) >= 2 and len(\" \".join(potential_name_parts)) < 40: # Avoid long sentences\n",
        "                    name = \" \".join(potential_name_parts)\n",
        "                    break # Found a plausible name, stop searching\n",
        "\n",
        "# --- RETURN: Only name and email ---\n",
        "    return {\"name\": name, \"email\": email}\n",
        "\n",
        "\n",
        "# --- Main Analysis Function ---\n",
        "def run_analysis(jd_file_name, resume_file_names):\n",
        "    \"\"\"\n",
        "    Main function to run the resume analysis.\n",
        "    Args:\n",
        "        jd_file_name (str): The filename of the job description in the JobDescriptions folder.\n",
        "        resume_file_names (list): A list of filenames of resumes in the Resumes folder.\n",
        "    \"\"\"\n",
        "    print(f\"Starting analysis for JD: {jd_file_name}, Resumes: {resume_file_names}\")\n",
        "\n",
        "    # 1. Read Job Description\n",
        "    jd_path = os.path.join(JD_FOLDER, jd_file_name)\n",
        "    jd_text = read_file_content(jd_path)\n",
        "    if not jd_text:\n",
        "        print(f\"Error: Could not read job description from {jd_path}. Aborting analysis.\")\n",
        "        return {\"error\": \"Could not read job description.\"}\n",
        "\n",
        "    jd_skills = extract_skills(jd_text, common_skills)\n",
        "    print(f\"Extracted JD skills: {jd_skills}\")\n",
        "\n",
        "    candidates_data = []\n",
        "    all_extracted_skills_overall = [] # To collect all skills for the general word cloud\n",
        "\n",
        "    # 2. Process Each Resume\n",
        "    for resume_name in resume_file_names:\n",
        "        resume_path = os.path.join(RESUMES_FOLDER, resume_name)\n",
        "        resume_text = read_file_content(resume_path)\n",
        "        if not resume_text:\n",
        "            print(f\"Warning: Could not read resume from {resume_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Extract Contact Info ---\n",
        "        contact_info = extract_contact_info(resume_text)\n",
        "        candidate_name = contact_info[\"name\"]\n",
        "        candidate_email = contact_info[\"email\"]\n",
        "        print(f\"Extracted info for {resume_name}: Name='{candidate_name}', Email='{candidate_email}'\")\n",
        "\n",
        "\n",
        "        resume_skills = extract_skills(resume_text, common_skills)\n",
        "        score = calculate_score(jd_skills, resume_skills)\n",
        "        summary = summarize_resume(resume_text, jd_text)\n",
        "\n",
        "        candidates_data.append({\n",
        "            \"resume_name\": resume_name,\n",
        "            \"candidate_name\": candidate_name, # Name\n",
        "            \"email\": candidate_email,         # Email\n",
        "            \"score\": round(score, 2), # Round score for cleaner display\n",
        "            \"summary\": summary,\n",
        "            \"skills\": resume_skills # Skills extracted from this specific resume\n",
        "        })\n",
        "        all_extracted_skills_overall.extend(resume_skills) # Add to overall list\n",
        "\n",
        "    # 3. Sort Candidates and Select Top 3\n",
        "    candidates_data.sort(key=lambda x: x['score'], reverse=True)\n",
        "    top_3_candidates = candidates_data[:3]\n",
        "    print(f\"Top 3 candidates: {top_3_candidates}\")\n",
        "\n",
        "    # 4. Prepare Data for Visualizations\n",
        "\n",
        "    # For Word Cloud: Count frequencies of all extracted skills across all resumes\n",
        "    skill_counts_for_wordcloud = Counter(all_extracted_skills_overall)\n",
        "    word_cloud_data = [{\"text\": skill, \"value\": count} for skill, count in skill_counts_for_wordcloud.items()]\n",
        "    print(f\"Word cloud data generated.\")\n",
        "\n",
        "    # For Radar Chart: Skill presence for top 3 candidates\n",
        "    radar_chart_data = []\n",
        "    for candidate in top_3_candidates:\n",
        "        candidate_skill_presence = {skill: 1 if skill in candidate['skills'] else 0 for skill in common_skills}\n",
        "        radar_chart_data.append({\n",
        "            \"name\": candidate['candidate_name'], # Use extracted name for radar chart\n",
        "            \"skills\": candidate_skill_presence # Dictionary of skill: 0/1 presence\n",
        "        })\n",
        "    print(f\"Radar chart data generated.\")\n",
        "\n",
        "    # 5. Compile and Save Results\n",
        "    results = {\n",
        "        \"top_candidates\": top_3_candidates,\n",
        "        \"skill_word_cloud\": word_cloud_data,\n",
        "        \"radar_chart_data\": radar_chart_data\n",
        "    }\n",
        "\n",
        "    results_file_path = os.path.join(RESULTS_FOLDER, RESULTS_FILE_NAME)\n",
        "    os.makedirs(RESULTS_FOLDER, exist_ok=True) # Ensure results folder exists\n",
        "    with open(results_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Analysis complete and results saved to {results_file_path}\")\n",
        "\n",
        "    return results # Return results for potential direct use or logging\n",
        "\n",
        "\n",
        "# --- Entry Point for n8n (receives parameters from n8n) ---\n",
        "# This block will be executed by n8n.\n",
        "# n8n's Colab node allows you to pass parameters as key-value pairs.\n",
        "# These parameters will be available as variables in the Colab environment.\n",
        "\n",
        "# Ensure these variables are defined when n8n runs the notebook\n",
        "# If running locally for testing, define them here:\n",
        "if 'jd_file_name' not in locals():\n",
        "    print(\"Running in local test mode. Please ensure 'jd_file_name' and 'resume_file_names' are defined.\")\n",
        "    # Example for local testing:\n",
        "    # Create dummy files in your Google Drive folders for testing\n",
        "    # with open(os.path.join(JD_FOLDER, 'example_job_description.txt'), 'w') as f:\n",
        "    #     f.write('We are looking for a Python developer with strong data analysis skills and experience in machine learning.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_1.pdf'), 'w') as f:\n",
        "    #     f.write('This is a dummy PDF content for Resume 1. Python, data analysis, machine learning experience. John Doe, john.doe@example.com.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_2.docx'), 'w') as f:\n",
        "    #     f.write('This is a dummy DOCX content for Resume 2. Java developer, some Python. Jane Smith, jane.smith@email.net.')\n",
        "    jd_file_name = 'example_job_description.pdf'\n",
        "    resume_file_names = ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
        "\n",
        "\n",
        "# Run the analysis\n",
        "if 'jd_file_name' in locals() and 'resume_file_names' in locals():\n",
        "    try:\n",
        "        run_analysis(jd_file_name, resume_file_names)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during analysis execution: {e}\")\n",
        "else:\n",
        "    print(\"Colab notebook executed without required parameters (jd_file_name, resume_file_names).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "GfUIFFWBeeL9",
        "outputId": "9c6b8ee1-3337-409c-86c5-f2bb1757cda9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in local test mode. Please ensure 'jd_file_name' and 'resume_file_names' are defined.\n",
            "Starting analysis for JD: example_job_description.pdf, Resumes: ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
            "Extracted JD skills: ['java', 'php', 'ai models', 'aws', 'azure', 'rag', 'go', 'artificial intelligence', 'low-code', 'excel', 'documentation', 'python', 'git', 'google cloud', 'process optimization']\n",
            "Extracted info for example_resume_1.pdf: Name='TejaswiSaiKumar Parepalli', Email='N/A'\n",
            "Extracted info for example_resume_2.pdf: Name='VASANTHA LAKSHMI EDA', Email='vasanthaeda982@gmail.com'\n",
            "Extracted info for example_resume_3.pdf: Name='SHIVOM MOYADE', Email='moyadeshivom@gmail.com'\n",
            "Extracted info for example_resume_4.pdf: Name='Jordan Perrone', Email='jperrone27@gmail.com'\n",
            "Extracted info for example_resume_5.pdf: Name='JONATHAN THOMAS', Email='jrthomasoff@gmail.com'\n",
            "Extracted info for example_resume_6.pdf: Name='Ambedkar Gorre', Email='gambedkar529@gmail.com'\n",
            "Top 3 candidates: [{'resume_name': 'example_resume_2.pdf', 'candidate_name': 'VASANTHA LAKSHMI EDA', 'email': 'vasanthaeda982@gmail.com', 'score': 28.12, 'summary': \"Vasantha Lakshmi EDA is a Data Analyst with 4+ years of experience in data analysis, machine learning, and cloud technologies (AWS, Snowflake, Databricks).  Her skills encompass Python, SQL, various data visualization tools (Tableau, Power BI), and ETL processes, demonstrated through projects resulting in significant improvements in data accuracy, efficiency, and decision-making across multiple industries.  She holds a Master's in Information Technology and multiple relevant certifications.\\n\", 'skills': ['aws', 'ci/cd', 'pandas', 'documentation', 'seaborn', 'numpy', 'python', 'agile', 'git', 'critical thinking', 'tensorflow', 'java', 'jira', 'automation', 'azure', 'sql', 'go', 'matplotlib', 'excel', 'machine learning', 'rag', 'data analysis', 'communication', 'leadership', 'data science', 'flask']}, {'resume_name': 'example_resume_4.pdf', 'candidate_name': 'Jordan Perrone', 'email': 'jperrone27@gmail.com', 'score': 25.0, 'summary': \"Jordan Perrone is a highly skilled AI specialist with a Master's degree in Artificial Intelligence from FAU (anticipated graduation August 2025) and extensive experience in process optimization and energy systems.  His expertise includes Python, R, data analysis, and AI model development, demonstrated through projects automating energy storage system design and leading technical negotiations on multi-million dollar projects.  He possesses strong problem-solving skills and a proven ability to improve efficiency and profitability in engineering and construction projects.\\n\", 'skills': ['java', 'automation', 'rag', 'deep learning', 'go', 'artificial intelligence', 'communication', 'project management', 'natural language processing', 'data science', 'excel', 'training', 'react', 'troubleshooting', 'python']}, {'resume_name': 'example_resume_3.pdf', 'candidate_name': 'SHIVOM MOYADE', 'email': 'moyadeshivom@gmail.com', 'score': 22.0, 'summary': \"Shivom Moyade is a data science professional with a Master's in Business Analytics and Project Management, specializing in AI and possessing extensive experience in developing and deploying AI solutions, particularly LLMs and chatbots.  His experience includes leveraging cloud platforms (AWS, Azure), programming languages (Python, R), and various AI/ML techniques (NLP, transformer models, ETL processes) to optimize business processes and enhance data analysis.  He has a strong track record of successfully completing projects with measurable results, demonstrated through quantifiable achievements in previous roles.\\n\", 'skills': ['aws', 'ci/cd', 'deep learning', 'pandas', 'kubernetes', 'artificial intelligence', 'devops', 'customer service', 'seaborn', 'documentation', 'numpy', 'python', 'agile', 'git', 'google cloud', 'tensorflow', 'java', 'jira', 'mysql', 'azure', 'nosql', 'sql', 'go', 'scrum', 'natural language processing', 'word', 'c++', 'pytorch', 'vector databases', 'project management', 'prompt engineering', 'javascript', 'excel', 'teamwork', 'microsoft office', 'machine learning', 'docker', 'rag', 'data analysis', 'communication', 'leadership', 'nlp', 'data science', 'generative ai', 'feature engineering', 'flask']}]\n",
            "Word cloud data generated.\n",
            "Radar chart data generated.\n",
            "Analysis complete and results saved to /content/drive/MyDrive/ResumeScorerApp/Results/analysis_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KboXVuYpktbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}