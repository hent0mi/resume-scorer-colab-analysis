{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EbECtZlCmP8r0ySHR-Nnd9cECMMzTaFq",
      "authorship_tag": "ABX9TyPFN2jVt9wsGQizlNlnOQRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hent0mi/resume-scorer-colab-analysis/blob/main/ResumeAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZgVVTbVLjYM",
        "outputId": "b6bebcb0-705a-4f98-932e-c1d647a58f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Optional: Verify the mount by listing your app folder\n",
        "# !ls /content/drive/MyDrive/ResumeScorerApp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk spacy PyPDF2 python-docx google-generativeai\n",
        "!python -m spacy download en_core_web_sm # Download a small English model for spaCy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFc8xzovNprh",
        "outputId": "7a0e8cc3-fe3c-44e6-f8d8-39786a1a6c09",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re # Import the regular expression module\n",
        "import asyncio # For async API calls\n",
        "import os\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import spacy\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Install nest_asyncio if not already installed\n",
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Apply this at the beginning of your script/notebook\n",
        "\n",
        "# --- Existing setup from previous steps ---\n",
        "# Load a small English language model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Configure Gemini API (assuming GEMINI_API_KEY is set in Colab Secrets)\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY secret not found or not enabled for this notebook.\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Define a broad list of common IT skills\n",
        "common_skills = [\n",
        "    # Core Programming & Data (Existing)\n",
        "    \"python\", \"java\", \"javascript\", \"c++\", \"c#\", \"go\", \"ruby\", \"php\", \"swift\", \"kotlin\",\n",
        "    \"sql\", \"nosql\", \"mongodb\", \"postgresql\", \"mysql\", \"redis\",\n",
        "    \"data analysis\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\",\n",
        "\n",
        "    # Cloud & DevOps (Existing)\n",
        "    \"aws\", \"azure\", \"google cloud\", \"docker\", \"kubernetes\", \"terraform\",\n",
        "    \"git\", \"jira\", \"agile\", \"scrum\", \"devops\", \"ci/cd\",\n",
        "\n",
        "    # General Business & Soft Skills (Existing)\n",
        "    \"project management\", \"leadership\", \"communication\", \"teamwork\", \"problem-solving\",\n",
        "    \"microsoft office\", \"excel\", \"word\", \"powerpoint\", \"outlook\",\n",
        "    \"customer service\",\n",
        "\n",
        "    # Web Development (Existing)\n",
        "    \"react\", \"angular\", \"vue.js\", \"node.js\", \"django\", \"flask\", \"spring\", \"laravel\",\n",
        "    \"web development\", \"frontend\", \"backend\", \"fullstack\", \"api development\", \"restful apis\",\n",
        "    \"ui/ux design\", \"figma\", \"sketch\", \"adobe xd\",\n",
        "\n",
        "    # Generative AI Specific Skills\n",
        "    \"generative ai\", \"genai\", \"artificial intelligence\", \"large language models\", \"llms\", \"ai models\",\n",
        "    \"prompt engineering\", \"prompting\", \"ai ethics\", \"model evaluation\", \"fine-tuning\",\n",
        "    \"retrieval augmented generation\", \"rag\", \"embeddings\", \"vector databases\",\n",
        "\n",
        "    # Process Optimization & Automation\n",
        "    \"process optimization\", \"business process automation\", \"bpa\", \"workflow automation\",\n",
        "    \"process analysis\", \"lean six sigma\", \"efficiency improvement\", \"automation\",\n",
        "    \"robotics process automation\", \"rpa\", \"business analysis\",\n",
        "\n",
        "    # Low-Code/No-Code Platforms\n",
        "    \"low-code\", \"no-code\", \"power platform\", \"microsoft power automate\", \"zapier\", \"make.com\",\n",
        "    \"bubble.io\", \"webflow\", \"appian\", \"outsystems\", \"mendix\", \"salesforce flow\",\n",
        "\n",
        "    # Analytical & Troubleshooting\n",
        "    \"troubleshooting\", \"root cause analysis\", \"diagnostic skills\", \"analytical thinking\",\n",
        "    \"data interpretation\", \"critical thinking\", \"problem diagnosis\",\n",
        "\n",
        "    # AI/ML Fundamentals (Expanded from generic ML)\n",
        "    \"machine learning\", \"deep learning\", \"nlp\", \"natural language processing\", \"computer vision\",\n",
        "    \"tensorflow\", \"pytorch\", \"scikit-learn\", \"data science\", \"model deployment\",\n",
        "    \"model monitoring\", \"feature engineering\", \"statistical analysis\",\n",
        "\n",
        "    # Communication & Training (for the analyst role)\n",
        "    \"training\", \"user training\", \"documentation\", \"technical writing\", \"presentations\",\n",
        "    \"stakeholder management\", \"user adoption\", \"change management\"\n",
        "]\n",
        "\n",
        "# Define base paths for Google Drive folders\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/ResumeScorerApp/'\n",
        "JD_FOLDER = os.path.join(DRIVE_BASE_PATH, 'JobDescriptions')\n",
        "RESUMES_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Resumes')\n",
        "RESULTS_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Results')\n",
        "RESULTS_FILE_NAME = 'analysis_results.json' # Fixed name for easy retrieval by n8n\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "# File Reading Functions: Create functions to read content from PDF, DOCX, and TXT files (read_text_from_pdf, read_text_from_docx, read_file_content).\n",
        "def read_text_from_pdf(filepath):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_text_from_docx(filepath):\n",
        "    \"\"\"Extracts text from a DOCX file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = Document(filepath)\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            full_text.append(para.text)\n",
        "        text = '\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_file_content(filepath):\n",
        "    \"\"\"Determines file type and calls appropriate reader function.\"\"\"\n",
        "    if filepath.endswith('.pdf'):\n",
        "        return read_text_from_pdf(filepath)\n",
        "    elif filepath.endswith('.docx'):\n",
        "        return read_text_from_docx(filepath)\n",
        "    elif filepath.endswith('.txt'):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading TXT {filepath}: {e}\")\n",
        "            return \"\"\n",
        "    return \"\" # Return empty string for unsupported types\n",
        "\n",
        "# Skill Extraction: spaCy will be used for basic keyword matching against a comprehensive list of skills. (extract_skills)\n",
        "def extract_skills(text, skill_keywords_list):\n",
        "    \"\"\"Extracts skills from text based on a predefined keyword list.\"\"\"\n",
        "    if nlp is None: # Check if nlp model loaded successfully\n",
        "        print(\"Warning: spaCy NLP model not loaded. Skill extraction will be limited.\")\n",
        "        return [] # Return empty list if NLP is not available\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "    found_skills = set()\n",
        "    for skill in skill_keywords_list:\n",
        "        if skill.lower() in doc.text:\n",
        "            found_skills.add(skill)\n",
        "    return list(found_skills)\n",
        "\n",
        "#  Scoring Algorithm: A basic Jaccard similarity based on shared skills. (calculate_score)\n",
        "def calculate_score(jd_skills, resume_skills):\n",
        "    \"\"\"Calculates a compatibility score based on shared skills.\"\"\"\n",
        "    if not jd_skills or not resume_skills:\n",
        "        return 0.0 # No skills to compare\n",
        "    jd_set = set(jd_skills)\n",
        "    resume_set = set(resume_skills)\n",
        "    intersection = len(jd_set.intersection(resume_set))\n",
        "    union = len(jd_set.union(resume_set))\n",
        "    return (intersection / union) * 100 if union > 0 else 0.0 # Score out of 100\n",
        "\n",
        "# Resume Summarization: Use the Gemini API to generate concise summaries.(summarize_resume)\n",
        "async def summarize_resume(resume_text, job_description_text):\n",
        "    \"\"\"Generates a concise summary of a resume relevant to a job description using Gemini.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Use gemini-1.5-flash for speed and cost-effectiveness\n",
        "    prompt = f\"\"\"Summarize the following resume in 2-3 concise sentences, focusing on skills, experience, and qualifications that are most relevant to the provided job description.\n",
        "\n",
        "    Job Description:\n",
        "    {job_description_text}\n",
        "\n",
        "    Resume:\n",
        "    {resume_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await model.generate_content_async(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing resume with Gemini: {e}\")\n",
        "        return \"Summary unavailable due to processing error.\"\n",
        "\n",
        "# Extract Contact Information: Extracting personal information like name, email, and address (extract_contact_info)\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"\n",
        "    Extracts candidate name, email, and a simplified address from resume text.\n",
        "    This is a heuristic approach and may not be 100% accurate.\n",
        "    \"\"\"\n",
        "    name = \"N/A\"\n",
        "    email = \"N/A\"\n",
        "    address = \"N/A\"\n",
        "\n",
        "    # 1. Extract Email (most reliable with regex)\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    if emails:\n",
        "        email = emails[0] # Take the first email found\n",
        "\n",
        "    # 2. Extract Name (very challenging without advanced NLP)\n",
        "    # This is a very basic heuristic: often the first few lines of a resume contain the name.\n",
        "    # We'll try to find a capitalized phrase at the beginning.\n",
        "    lines = text.strip().split('\\n')\n",
        "    if lines:\n",
        "        first_line = lines[0].strip()\n",
        "        # Look for a line that seems like a name (e.g., \"John Doe\" or \"J. Doe\")\n",
        "        # This regex looks for 2-4 capitalized words, potentially with periods.\n",
        "        name_pattern = r'^[A-Z][a-z]+(?: [A-Z][a-z]+){1,3}$|^[A-Z]\\. [A-Z][a-z]+(?: [A-Z][a-z]+)?$'\n",
        "        if re.match(name_pattern, first_line):\n",
        "            name = first_line\n",
        "        else:\n",
        "            # Fallback: try to find a capitalized phrase in the first few lines\n",
        "            for i in range(min(len(lines), 3)): # Check first 3 lines\n",
        "                line = lines[i].strip()\n",
        "                # Simple check for multiple capitalized words\n",
        "                potential_name_parts = [word for word in line.split() if word and word[0].isupper()]\n",
        "                if len(potential_name_parts) >= 2 and len(\" \".join(potential_name_parts)) < 40: # Avoid long sentences\n",
        "                    name = \" \".join(potential_name_parts)\n",
        "                    break # Found a plausible name, stop searching\n",
        "\n",
        "    # 3. Extract Address (also challenging, highly variable formats)\n",
        "    # This is a very simplified approach, looking for common address components.\n",
        "    # It will likely extract only parts of an address or common city/state/zip patterns.\n",
        "    address_keywords = ['street', 'avenue', 'road', 'lane', 'drive', 'boulevard',\n",
        "                        'st', 'ave', 'rd', 'ln', 'dr', 'blvd',\n",
        "                        'city', 'state', 'zip', 'postal code', 'united states', 'usa',\n",
        "                        r'\\d{5}(-\\d{4})?', # US Zip code\n",
        "                        r'[A-Z]{2}\\s+\\d{5}', # State Abbreviation + Zip\n",
        "                        r'[A-Z][a-z]+,?\\s+[A-Z]{2}\\s+\\d{5}' # City, State Zip\n",
        "                       ]\n",
        "    found_address_parts = []\n",
        "    for keyword in address_keywords:\n",
        "        if isinstance(keyword, str):\n",
        "            if keyword.lower() in text.lower():\n",
        "                # Find the first occurrence of a keyword and try to extract surrounding text\n",
        "                match = re.search(r'(.{0,50}' + re.escape(keyword) + r'.{0,50})', text, re.IGNORECASE | re.DOTALL)\n",
        "                if match:\n",
        "                    found_address_parts.append(match.group(1).strip())\n",
        "        else: # It's a regex pattern\n",
        "            matches = re.findall(keyword, text)\n",
        "            if matches:\n",
        "                found_address_parts.extend(matches)\n",
        "\n",
        "    if found_address_parts:\n",
        "        # Try to combine unique parts, but this is still very rough\n",
        "        address = \", \".join(list(set(found_address_parts)))\n",
        "        # Further refine by trying to get a full line that contains an address component\n",
        "        for line in lines:\n",
        "            if any(re.search(re.escape(k), line, re.IGNORECASE) if isinstance(k, str) else re.search(k, line) for k in address_keywords):\n",
        "                if len(line) < 150: # Avoid very long lines\n",
        "                    address = line.strip()\n",
        "                    break # Take the first plausible full line\n",
        "\n",
        "    return {\"name\": name, \"email\": email, \"address\": address}\n",
        "\n",
        "\n",
        "# --- Main Analysis Function ---\n",
        "async def run_analysis(jd_file_name, resume_file_names):\n",
        "    \"\"\"\n",
        "    Main function to run the resume analysis.\n",
        "    Args:\n",
        "        jd_file_name (str): The filename of the job description in the JobDescriptions folder.\n",
        "        resume_file_names (list): A list of filenames of resumes in the Resumes folder.\n",
        "    \"\"\"\n",
        "    print(f\"Starting analysis for JD: {jd_file_name}, Resumes: {resume_file_names}\")\n",
        "\n",
        "    # 1. Read Job Description\n",
        "    jd_path = os.path.join(JD_FOLDER, jd_file_name)\n",
        "    jd_text = read_file_content(jd_path)\n",
        "    if not jd_text:\n",
        "        print(f\"Error: Could not read job description from {jd_path}. Aborting analysis.\")\n",
        "        return {\"error\": \"Could not read job description.\"}\n",
        "\n",
        "    jd_skills = extract_skills(jd_text, common_skills)\n",
        "    print(f\"Extracted JD skills: {jd_skills}\")\n",
        "\n",
        "    candidates_data = []\n",
        "    all_extracted_skills_overall = [] # To collect all skills for the general word cloud\n",
        "\n",
        "    # 2. Process Each Resume\n",
        "    for resume_name in resume_file_names:\n",
        "        resume_path = os.path.join(RESUMES_FOLDER, resume_name)\n",
        "        resume_text = read_file_content(resume_path)\n",
        "        if not resume_text:\n",
        "            print(f\"Warning: Could not read resume from {resume_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Extract Contact Info ---\n",
        "        contact_info = extract_contact_info(resume_text)\n",
        "        candidate_name = contact_info[\"name\"]\n",
        "        candidate_email = contact_info[\"email\"]\n",
        "        candidate_address = contact_info[\"address\"]\n",
        "        print(f\"Extracted info for {resume_name}: Name='{candidate_name}', Email='{candidate_email}', Address='{candidate_address}'\")\n",
        "\n",
        "\n",
        "        resume_skills = extract_skills(resume_text, common_skills)\n",
        "        score = calculate_score(jd_skills, resume_skills)\n",
        "        summary = await summarize_resume(resume_text, jd_text)\n",
        "\n",
        "        candidates_data.append({\n",
        "            \"resume_name\": resume_name,\n",
        "            \"candidate_name\": candidate_name, # Name\n",
        "            \"email\": candidate_email,         # Email\n",
        "            \"address\": candidate_address,     # Address\n",
        "            \"score\": round(score, 2), # Round score for cleaner display\n",
        "            \"summary\": summary,\n",
        "            \"skills\": resume_skills # Skills extracted from this specific resume\n",
        "        })\n",
        "        all_extracted_skills_overall.extend(resume_skills) # Add to overall list\n",
        "\n",
        "    # 3. Sort Candidates and Select Top 3\n",
        "    candidates_data.sort(key=lambda x: x['score'], reverse=True)\n",
        "    top_3_candidates = candidates_data[:3]\n",
        "    print(f\"Top 3 candidates: {top_3_candidates}\")\n",
        "\n",
        "    # 4. Prepare Data for Visualizations\n",
        "\n",
        "    # For Word Cloud: Count frequencies of all extracted skills across all resumes\n",
        "    skill_counts_for_wordcloud = Counter(all_extracted_skills_overall)\n",
        "    word_cloud_data = [{\"text\": skill, \"value\": count} for skill, count in skill_counts_for_wordcloud.items()]\n",
        "    print(f\"Word cloud data generated.\")\n",
        "\n",
        "    # For Radar Chart: Skill presence for top 3 candidates\n",
        "    radar_chart_data = []\n",
        "    for candidate in top_3_candidates:\n",
        "        candidate_skill_presence = {skill: 1 if skill in candidate['skills'] else 0 for skill in common_skills}\n",
        "        radar_chart_data.append({\n",
        "            \"name\": candidate['candidate_name'], # Use extracted name for radar chart\n",
        "            \"skills\": candidate_skill_presence # Dictionary of skill: 0/1 presence\n",
        "        })\n",
        "    print(f\"Radar chart data generated.\")\n",
        "\n",
        "    # 5. Compile and Save Results\n",
        "    results = {\n",
        "        \"top_candidates\": top_3_candidates,\n",
        "        \"skill_word_cloud\": word_cloud_data,\n",
        "        \"radar_chart_data\": radar_chart_data\n",
        "    }\n",
        "\n",
        "    results_file_path = os.path.join(RESULTS_FOLDER, RESULTS_FILE_NAME)\n",
        "    os.makedirs(RESULTS_FOLDER, exist_ok=True) # Ensure results folder exists\n",
        "    with open(results_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Analysis complete and results saved to {results_file_path}\")\n",
        "\n",
        "    return results # Return results for potential direct use or logging\n",
        "\n",
        "\n",
        "# --- Entry Point for n8n (unchanged, as it receives parameters from n8n) ---\n",
        "# This block will be executed by n8n.\n",
        "# n8n's Colab node allows you to pass parameters as key-value pairs.\n",
        "# These parameters will be available as variables in the Colab environment.\n",
        "\n",
        "# Ensure these variables are defined when n8n runs the notebook\n",
        "# If running locally for testing, define them here:\n",
        "if 'jd_file_name' not in locals():\n",
        "    print(\"Running in local test mode. Please ensure 'jd_file_name' and 'resume_file_names' are defined.\")\n",
        "    # Example for local testing:\n",
        "    # Create dummy files in your Google Drive folders for testing\n",
        "    # with open(os.path.join(JD_FOLDER, 'example_job_description.txt'), 'w') as f:\n",
        "    #     f.write('We are looking for a Python developer with strong data analysis skills and experience in machine learning.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_1.pdf'), 'w') as f:\n",
        "    #     f.write('This is a dummy PDF content for Resume 1. Python, data analysis, machine learning experience. John Doe, john.doe@example.com, 123 Main St, Anytown, CA 90210.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_2.docx'), 'w') as f:\n",
        "    #     f.write('This is a dummy DOCX content for Resume 2. Java developer, some Python. Jane Smith, jane.smith@email.net, 456 Oak Ave, Otherville, NY 10001.')\n",
        "    jd_file_name = 'example_job_description.pdf'\n",
        "    resume_file_names = ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
        "\n",
        "\n",
        "# Run the analysis\n",
        "if 'jd_file_name' in locals() and 'resume_file_names' in locals():\n",
        "    try:\n",
        "        asyncio.run(run_analysis(jd_file_name, resume_file_names))\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during analysis execution: {e}\")\n",
        "else:\n",
        "    print(\"Colab notebook executed without required parameters (jd_file_name, resume_file_names).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "GfUIFFWBeeL9",
        "outputId": "100cd439-0ec2-4c5d-8f5c-30d9ab25a931"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Running in local test mode. Please ensure 'jd_file_name' and 'resume_file_names' are defined.\n",
            "Starting analysis for JD: example_job_description.pdf, Resumes: ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
            "Extracted JD skills: ['excel', 'aws', 'low-code', 'documentation', 'git', 'azure', 'process optimization', 'java', 'rag', 'go', 'php', 'python', 'google cloud', 'artificial intelligence', 'ai models']\n",
            "Extracted info for example_resume_1.pdf: Name='TejaswiSaiKumar Parepalli', Email='N/A', Address='Results -driven Data Scientist  with expertise in architecting scalable ML systems and developing production data'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Extracted info for example_resume_2.pdf: Name='VASANTHA LAKSHMI EDA', Email='vasanthaeda982@gmail.com', Address='I am a Data Analyst with 4 years of experience in transforming raw data into actionable insights. Proficient in Python, SQL, and'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Extracted info for example_resume_3.pdf: Name='SHIVOM MOYADE', Email='moyadeshivom@gmail.com', Address='UNIVERSITY OF CONNECTICUT, SCHOOL OF BUSINESS , HARTFORD, CT                                       December 2024'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Extracted info for example_resume_4.pdf: Name='Jordan Perrone', Email='jperrone27@gmail.com', Address='Jordan Perrone'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Extracted info for example_resume_5.pdf: Name='JONATHAN THOMAS', Email='jrthomasoff@gmail.com', Address='Rainier Labs (startup), San Jose, CA: Robotics Software Engineer                 November 2024 - Present'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Extracted info for example_resume_6.pdf: Name='Ambedkar Gorre', Email='gambedkar529@gmail.com', Address='Master of Science in Data Science.'\n",
            "Error summarizing resume with Gemini: object GenerateContentResponse can't be used in 'await' expression\n",
            "Top 3 candidates: [{'resume_name': 'example_resume_2.pdf', 'candidate_name': 'VASANTHA LAKSHMI EDA', 'email': 'vasanthaeda982@gmail.com', 'address': 'I am a Data Analyst with 4 years of experience in transforming raw data into actionable insights. Proficient in Python, SQL, and', 'score': 28.12, 'summary': 'Summary unavailable due to processing error.', 'skills': ['critical thinking', 'tensorflow', 'flask', 'automation', 'machine learning', 'pandas', 'sql', 'matplotlib', 'aws', 'java', 'data science', 'data analysis', 'jira', 'communication', 'numpy', 'documentation', 'ci/cd', 'leadership', 'python', 'excel', 'seaborn', 'git', 'agile', 'azure', 'rag', 'go']}, {'resume_name': 'example_resume_4.pdf', 'candidate_name': 'Jordan Perrone', 'email': 'jperrone27@gmail.com', 'address': 'Jordan Perrone', 'score': 25.0, 'summary': 'Summary unavailable due to processing error.', 'skills': ['excel', 'react', 'natural language processing', 'java', 'project management', 'rag', 'automation', 'data science', 'go', 'python', 'troubleshooting', 'artificial intelligence', 'deep learning', 'training', 'communication']}, {'resume_name': 'example_resume_3.pdf', 'candidate_name': 'SHIVOM MOYADE', 'email': 'moyadeshivom@gmail.com', 'address': 'UNIVERSITY OF CONNECTICUT, SCHOOL OF BUSINESS , HARTFORD, CT                                       December 2024', 'score': 22.0, 'summary': 'Summary unavailable due to processing error.', 'skills': ['c++', 'tensorflow', 'prompt engineering', 'word', 'teamwork', 'flask', 'machine learning', 'pandas', 'artificial intelligence', 'kubernetes', 'sql', 'generative ai', 'aws', 'natural language processing', 'feature engineering', 'devops', 'customer service', 'java', 'project management', 'nlp', 'data science', 'vector databases', 'data analysis', 'jira', 'communication', 'numpy', 'documentation', 'ci/cd', 'leadership', 'python', 'google cloud', 'nosql', 'mysql', 'javascript', 'pytorch', 'docker', 'microsoft office', 'excel', 'seaborn', 'git', 'agile', 'azure', 'rag', 'go', 'deep learning', 'scrum']}]\n",
            "Word cloud data generated.\n",
            "Radar chart data generated.\n",
            "Analysis complete and results saved to /content/drive/MyDrive/ResumeScorerApp/Results/analysis_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KboXVuYpktbG"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}