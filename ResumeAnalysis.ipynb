{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EbECtZlCmP8r0ySHR-Nnd9cECMMzTaFq",
      "authorship_tag": "ABX9TyORKbdqgCH1t8Leqlryimn5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hent0mi/resume-scorer-colab-analysis/blob/main/ResumeAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZgVVTbVLjYM",
        "outputId": "1bfaa0c8-29b9-4ef4-881f-78bf01f44b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Optional: Verify the mount by listing your app folder\n",
        "# !ls /content/drive/MyDrive/ResumeScorerApp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Verify the mount by listing your app folder\n",
        "!ls /content/drive/MyDrive/ResumeScorerApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy9W0klINiY9",
        "outputId": "82aeb5fa-0277-4a01-a1bc-a03733e84c4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobDescriptions  Results  Resumes  test.gdoc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk spacy PyPDF2 python-docx google-generativeai\n",
        "!python -m spacy download en_core_web_sm # Download a small English model for spaCy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFc8xzovNprh",
        "outputId": "3136711d-83df-407c-db18-fa37fe090956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-generativeai"
      ],
      "metadata": {
        "id": "hTgkyCK5QqrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Gemini API Setup:\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata # This library helps access Colab secrets\n",
        "\n",
        "# Retrieve the API key from Colab Secrets\n",
        "# The string 'GEMINI_API_KEY' must match the name you gave your secret in step 2.\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Check if the API key was retrieved successfully (important for debugging)\n",
        "if api_key is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY secret not found or not enabled for this notebook.\")\n",
        "else:\n",
        "    print(\"Gemini API key loaded successfully.\") # You can remove this line in production\n",
        "\n",
        "# Configure the google-generativeai library with your API key\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Optional: Test a simple interaction to verify the setup\n",
        "try:\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Using the faster flash model\n",
        "    response = model.generate_content(\"Hello, Gemini!\")\n",
        "    print(\"Gemini says:\", response.text)\n",
        "except Exception as e:\n",
        "    print(f\"Error testing Gemini API: {e}\")\n",
        "    print(\"Please ensure your API key is correct and has the necessary permissions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "xjIVXSEYQzxz",
        "outputId": "48192aa2-735a-400b-cc49-ecdd3594f790"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API key loaded successfully.\n",
            "Gemini says: Hello there!  How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Reading Functions: Create functions to read content from PDF, DOCX, and TXT files.\n",
        "\n",
        "import os\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "\n",
        "def read_text_from_pdf(filepath):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_text_from_docx(filepath):\n",
        "    \"\"\"Extracts text from a DOCX file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = Document(filepath)\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            full_text.append(para.text)\n",
        "        text = '\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX {filepath}: {e}\")\n",
        "        text = \"\" # Return empty string on error\n",
        "    return text\n",
        "\n",
        "def read_file_content(filepath):\n",
        "    \"\"\"Determines file type and calls appropriate reader function.\"\"\"\n",
        "    if filepath.endswith('.pdf'):\n",
        "        return read_text_from_pdf(filepath)\n",
        "    elif filepath.endswith('.docx'):\n",
        "        return read_text_from_docx(filepath)\n",
        "    elif filepath.endswith('.txt'):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading TXT {filepath}: {e}\")\n",
        "            return \"\"\n",
        "    return \"\" # Return empty string for unsupported types"
      ],
      "metadata": {
        "id": "dzqOZy4vRLUu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skill Extraction: Use spaCy for basic keyword matching. You'll need a comprehensive list of skills.\n",
        "\n",
        "import spacy\n",
        "# Load a small English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_skills(text, skill_keywords_list):\n",
        "    \"\"\"Extracts skills from text based on a predefined keyword list.\"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    found_skills = set()\n",
        "    # Simple keyword matching: checks if a skill keyword is present in the text\n",
        "    for skill in skill_keywords_list:\n",
        "        if skill.lower() in doc.text:\n",
        "            found_skills.add(skill)\n",
        "    # For more advanced extraction, you could use spaCy's entity recognition\n",
        "    # or train a custom NER model.\n",
        "    return list(found_skills)\n",
        "\n",
        "# Define a broad list of common skills. Expand this significantly for better results.\n",
        "common_skills = [\n",
        "    # Core Programming & Data (Existing)\n",
        "    \"python\", \"java\", \"javascript\", \"c++\", \"c#\", \"go\", \"ruby\", \"php\", \"swift\", \"kotlin\",\n",
        "    \"sql\", \"nosql\", \"mongodb\", \"postgresql\", \"mysql\", \"redis\",\n",
        "    \"data analysis\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\",\n",
        "\n",
        "    # Cloud & DevOps (Existing)\n",
        "    \"aws\", \"azure\", \"google cloud\", \"docker\", \"kubernetes\", \"terraform\",\n",
        "    \"git\", \"jira\", \"agile\", \"scrum\", \"devops\", \"ci/cd\",\n",
        "\n",
        "    # General Business & Soft Skills (Existing)\n",
        "    \"project management\", \"leadership\", \"communication\", \"teamwork\", \"problem-solving\",\n",
        "    \"microsoft office\", \"excel\", \"word\", \"powerpoint\", \"outlook\",\n",
        "    \"customer service\",\n",
        "\n",
        "    # Web Development (Existing)\n",
        "    \"react\", \"angular\", \"vue.js\", \"node.js\", \"django\", \"flask\", \"spring\", \"laravel\",\n",
        "    \"web development\", \"frontend\", \"backend\", \"fullstack\", \"api development\", \"restful apis\",\n",
        "    \"ui/ux design\", \"figma\", \"sketch\", \"adobe xd\",\n",
        "\n",
        "    # New: Generative AI Specific Skills\n",
        "    \"generative ai\", \"genai\", \"artificial intelligence\" \"large language models\", \"llms\", \"ai models\",\n",
        "    \"prompt engineering\", \"prompting\", \"ai ethics\", \"model evaluation\", \"fine-tuning\",\n",
        "    \"retrieval augmented generation\", \"rag\", \"embeddings\", \"vector databases\",\n",
        "\n",
        "    # New: Process Optimization & Automation\n",
        "    \"process optimization\", \"business process automation\", \"bpa\", \"workflow automation\",\n",
        "    \"process analysis\", \"lean six sigma\", \"efficiency improvement\", \"automation\",\n",
        "    \"robotics process automation\", \"rpa\", \"business analysis\",\n",
        "\n",
        "    # New: Low-Code/No-Code Platforms\n",
        "    \"low-code\", \"no-code\", \"power platform\", \"microsoft power automate\", \"zapier\", \"make.com\",\n",
        "    \"bubble.io\", \"webflow\", \"appian\", \"outsystems\", \"mendix\", \"salesforce flow\",\n",
        "\n",
        "    # New: Analytical & Troubleshooting\n",
        "    \"troubleshooting\", \"root cause analysis\", \"diagnostic skills\", \"analytical thinking\",\n",
        "    \"data interpretation\", \"critical thinking\", \"problem diagnosis\",\n",
        "\n",
        "    # New: AI/ML Fundamentals (Expanded from generic ML)\n",
        "    \"machine learning\", \"deep learning\", \"nlp\", \"natural language processing\", \"computer vision\",\n",
        "    \"tensorflow\", \"pytorch\", \"scikit-learn\", \"data science\", \"model deployment\",\n",
        "    \"model monitoring\", \"feature engineering\", \"statistical analysis\",\n",
        "\n",
        "    # New: Communication & Training (for the analyst role)\n",
        "    \"training\", \"user training\", \"documentation\", \"technical writing\", \"presentations\",\n",
        "    \"stakeholder management\", \"user adoption\", \"change management\"\n",
        "]"
      ],
      "metadata": {
        "id": "fYQwfOhPRgy0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scoring Algorithm: A basic Jaccard similarity based on shared skills.\n",
        "\n",
        "def calculate_score(jd_skills, resume_skills):\n",
        "    \"\"\"Calculates a compatibility score based on shared skills.\"\"\"\n",
        "    if not jd_skills or not resume_skills:\n",
        "        return 0.0 # No skills to compare\n",
        "    jd_set = set(jd_skills)\n",
        "    resume_set = set(resume_skills)\n",
        "    intersection = len(jd_set.intersection(resume_set))\n",
        "    union = len(jd_set.union(resume_set))\n",
        "    return (intersection / union) * 100 if union > 0 else 0.0 # Score out of 100"
      ],
      "metadata": {
        "id": "L8hNgvTWRugA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume Summarization (Google Gemini): Use the Gemini API to generate concise summaries.\n",
        "\n",
        "# import asyncio # For async API calls - no longer needed for synchronous version\n",
        "\n",
        "def summarize_resume(resume_text, job_description_text):\n",
        "    \"\"\"Generates a concise summary of a resume relevant to a job description using Gemini.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Use gemini-1.5-flash for speed and cost-effectiveness\n",
        "    prompt = f\"\"\"Summarize the following resume in 2-3 concise sentences, focusing on skills, experience, and qualifications that are most relevant to the provided job description.\n",
        "\n",
        "    Job Description:\n",
        "    {job_description_text}\n",
        "\n",
        "    Resume:\n",
        "    {resume_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the synchronous generate_content instead of generate_content_async\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing resume with Gemini: {e}\")\n",
        "        return \"Summary unavailable due to processing error.\""
      ],
      "metadata": {
        "id": "GYcY2lM_SA_X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Analysis Function: This function will orchestrate the entire process, accept file names as input, and save the results.\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter # For word cloud skill counts\n",
        "import asyncio # Keep asyncio import for potential future use if needed, but remove await from run_analysis\n",
        "\n",
        "# Define base paths for Google Drive folders\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/ResumeScorerApp/'\n",
        "JD_FOLDER = os.path.join(DRIVE_BASE_PATH, 'JobDescriptions')\n",
        "RESUMES_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Resumes')\n",
        "RESULTS_FOLDER = os.path.join(DRIVE_BASE_PATH, 'Results')\n",
        "RESULTS_FILE_NAME = 'analysis_results.json' # Fixed name for easy retrieval by n8n\n",
        "\n",
        "# Remove async from the function definition\n",
        "def run_analysis(jd_file_name, resume_file_names):\n",
        "    \"\"\"\n",
        "    Main function to run the resume analysis.\n",
        "    Args:\n",
        "        jd_file_name (str): The filename of the job description in the JobDescriptions folder.\n",
        "        resume_file_names (list): A list of filenames of resumes in the Resumes folder.\n",
        "    \"\"\"\n",
        "    print(f\"Starting analysis for JD: {jd_file_name}, Resumes: {resume_file_names}\")\n",
        "\n",
        "    # 1. Read Job Description\n",
        "    jd_path = os.path.join(JD_FOLDER, jd_file_name)\n",
        "    jd_text = read_file_content(jd_path)\n",
        "    if not jd_text:\n",
        "        print(f\"Error: Could not read job description from {jd_path}. Aborting analysis.\")\n",
        "        return {\"error\": \"Could not read job description.\"}\n",
        "\n",
        "    jd_skills = extract_skills(jd_text, common_skills)\n",
        "    print(f\"Extracted JD skills: {jd_skills}\")\n",
        "\n",
        "    candidates_data = []\n",
        "    all_extracted_skills_overall = [] # To collect all skills for the general word cloud\n",
        "\n",
        "    # 2. Process Each Resume\n",
        "    for resume_name in resume_file_names:\n",
        "        resume_path = os.path.join(RESUMES_FOLDER, resume_name)\n",
        "        resume_text = read_file_content(resume_path)\n",
        "        if not resume_text:\n",
        "            print(f\"Warning: Could not read resume from {resume_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        resume_skills = extract_skills(resume_text, common_skills)\n",
        "        score = calculate_score(jd_skills, resume_skills)\n",
        "        # Call the synchronous summarize_resume function\n",
        "        summary = summarize_resume(resume_text, jd_text)\n",
        "\n",
        "        candidates_data.append({\n",
        "            \"resume_name\": resume_name,\n",
        "            \"score\": round(score, 2), # Round score for cleaner display\n",
        "            \"summary\": summary,\n",
        "            \"skills\": resume_skills # Skills extracted from this specific resume\n",
        "        })\n",
        "        all_extracted_skills_overall.extend(resume_skills) # Add to overall list\n",
        "\n",
        "    # 3. Sort Candidates and Select Top 3\n",
        "    candidates_data.sort(key=lambda x: x['score'], reverse=True)\n",
        "    top_3_candidates = candidates_data[:3]\n",
        "    print(f\"Top 3 candidates: {top_3_candidates}\")\n",
        "\n",
        "    # 4. Prepare Data for Visualizations\n",
        "\n",
        "    # For Word Cloud: Count frequencies of all extracted skills across all resumes\n",
        "    skill_counts_for_wordcloud = Counter(all_extracted_skills_overall)\n",
        "    # Convert to a list of objects for easier JS consumption if needed, or keep as dict\n",
        "    # Example: [{\"text\": \"python\", \"value\": 10}, ...]\n",
        "    word_cloud_data = [{\"text\": skill, \"value\": count} for skill, count in skill_counts_for_wordcloud.items()]\n",
        "    print(f\"Word cloud data generated.\")\n",
        "\n",
        "    # For Radar Chart: Skill presence for top 3 candidates\n",
        "    radar_chart_data = []\n",
        "    for candidate in top_3_candidates:\n",
        "        candidate_skill_presence = {skill: 1 if skill in candidate['skills'] else 0 for skill in common_skills}\n",
        "        radar_chart_data.append({\n",
        "            \"name\": candidate['resume_name'],\n",
        "            \"skills\": candidate_skill_presence # Dictionary of skill: 0/1 presence\n",
        "        })\n",
        "    print(f\"Radar chart data generated.\")\n",
        "\n",
        "    # 5. Compile and Save Results\n",
        "    results = {\n",
        "        \"top_candidates\": top_3_candidates,\n",
        "        \"skill_word_cloud\": word_cloud_data,\n",
        "        \"radar_chart_data\": radar_chart_data\n",
        "    }\n",
        "\n",
        "    results_file_path = os.path.join(RESULTS_FOLDER, RESULTS_FILE_NAME)\n",
        "    os.makedirs(RESULTS_FOLDER, exist_ok=True) # Ensure results folder exists\n",
        "    with open(results_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Analysis complete and results saved to {results_file_path}\")\n",
        "\n",
        "    return results # Return results for potential direct use or logging"
      ],
      "metadata": {
        "id": "OXD7aO7USFjb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entry Point for n8n: The n8n Google Colaboratory node allows you to pass parameters to your notebook.\n",
        "# You'll need a way for your notebook to receive these. A common pattern is to use sys.argv for command-line arguments,\n",
        "# but for Colab, parameters are often set as variables directly in a cell that n8n executes.\n",
        "\n",
        "# This block will be executed by n8n.\n",
        "# n8n's Colab node allows you to pass parameters as key-value pairs.\n",
        "# These parameters will be available as variables in the Colab environment.\n",
        "# Example: If n8n passes 'jd_file_name' and 'resume_file_names'\n",
        "# you can access them directly here.\n",
        "\n",
        "# --- Placeholder for n8n-injected parameters ---\n",
        "# For local testing in Colab, you can uncomment and set these:\n",
        "# jd_file_name = 'example_job_description.pdf' # Make sure this file exists in your Drive JD folder\n",
        "# resume_file_names = ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf'] # Make sure these exist in your Drive Resumes folder\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Ensure these variables are defined when n8n runs the notebook\n",
        "# If running locally for testing, define them here:\n",
        "if 'jd_file_name' not in locals():\n",
        "    print(\"Running in local test mode. Please ensure 'jd_file_name' and 'resume_file_names' are defined.\")\n",
        "    # Example for local testing:\n",
        "    # Create dummy files in your Google Drive folders for testing\n",
        "    # with open(os.path.join(JD_FOLDER, 'example_job_description.txt'), 'w') as f:\n",
        "    #     f.write('We are looking for a Python developer with strong data analysis skills and experience in machine learning.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_1.pdf'), 'w') as f:\n",
        "    #     f.write('This is a dummy PDF content for Resume 1. Python, data analysis, machine learning experience.')\n",
        "    # with open(os.path.join(RESUMES_FOLDER, 'example_resume_2.docx'), 'w') as f:\n",
        "    #     f.write('This is a dummy DOCX content for Resume 2. Java developer, some Python.')\n",
        "    jd_file_name = 'example_job_description.pdf'\n",
        "    resume_file_names = ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
        "\n",
        "\n",
        "# Run the analysis\n",
        "if 'jd_file_name' in locals() and 'resume_file_names' in locals():\n",
        "    try:\n",
        "        # Use await instead of asyncio.run() when running within Colab\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        await run_analysis(jd_file_name, resume_file_names)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during analysis execution: {e}\")\n",
        "else:\n",
        "    print(\"Colab notebook executed without required parameters (jd_file_name, resume_file_names).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "_HgA7Gq4SY2n",
        "outputId": "a7d8e0da-eab5-410a-9aff-63b2ee3f9fca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis for JD: example_job_description.pdf, Resumes: ['example_resume_1.pdf', 'example_resume_2.pdf', 'example_resume_3.pdf', 'example_resume_4.pdf', 'example_resume_5.pdf', 'example_resume_6.pdf']\n",
            "Extracted JD skills: ['php', 'go', 'google cloud', 'excel', 'rag', 'low-code', 'documentation', 'java', 'ai models', 'aws', 'git', 'azure', 'python', 'process optimization']\n",
            "Top 3 candidates: [{'resume_name': 'example_resume_2.pdf', 'score': 29.03, 'summary': 'Vasantha Lakshmi EDA is a Data Analyst with 4+ years of experience leveraging Python, SQL, AWS, and various visualization tools (Tableau, Power BI) to create impactful dashboards and optimize data workflows.  Her expertise includes developing and deploying machine learning models (TensorFlow, scikit-learn), ETL processes, and data cleaning/processing of large datasets (over 10 million records).  Proven success in improving data accuracy, reducing operational costs, and enhancing decision-making across multiple business units.\\n', 'skills': ['agile', 'automation', 'sql', 'rag', 'machine learning', 'aws', 'git', 'numpy', 'azure', 'data science', 'communication', 'tensorflow', 'documentation', 'java', 'python', 'data analysis', 'ci/cd', 'go', 'excel', 'leadership', 'matplotlib', 'flask', 'seaborn', 'critical thinking', 'pandas', 'jira']}, {'resume_name': 'example_resume_4.pdf', 'score': 21.74, 'summary': \"Jordan Perrone is a highly skilled AI specialist with a Master's degree in Artificial Intelligence from FAU (anticipated graduation August 2025) and extensive experience in process optimization, particularly within the energy sector.  His expertise includes Python programming, AI model development and deployment, data analysis using tools like PVsyst and Excel, and experience with cloud platforms.  His resume highlights significant achievements in automating engineering processes, leading technical negotiations, and improving efficiency in large-scale projects.\\n\", 'skills': ['data science', 'go', 'excel', 'rag', 'deep learning', 'java', 'automation', 'troubleshooting', 'python', 'communication', 'natural language processing', 'react', 'project management', 'training']}, {'resume_name': 'example_resume_3.pdf', 'score': 20.41, 'summary': \"Shivom Moyade is a highly skilled data scientist and AI engineer with a Master's in Business Analytics and Project Management, specializing in data science.  His experience includes developing and deploying LLMs, implementing ETL processes on AWS, and using various AI/ML techniques (e.g., BERT, NLP,  regression algorithms) to improve business processes and achieve significant results (e.g., 25% workflow efficiency increase).  His expertise in Python, cloud platforms (AWS, Azure, GCP), and database technologies aligns well with the requirements of the Process Optimization AI Specialist position.\\n\", 'skills': ['agile', 'google cloud', 'microsoft office', 'feature engineering', 'pytorch', 'generative ai', 'prompt engineering', 'scrum', 'sql', 'deep learning', 'rag', 'machine learning', 'docker', 'aws', 'git', 'numpy', 'azure', 'data science', 'communication', 'customer service', 'tensorflow', 'teamwork', 'nlp', 'c++', 'documentation', 'word', 'java', 'nosql', 'devops', 'python', 'natural language processing', 'data analysis', 'javascript', 'project management', 'ci/cd', 'go', 'excel', 'leadership', 'kubernetes', 'flask', 'mysql', 'seaborn', 'vector databases', 'pandas', 'jira']}]\n",
            "Word cloud data generated.\n",
            "Radar chart data generated.\n",
            "Analysis complete and results saved to /content/drive/MyDrive/ResumeScorerApp/Results/analysis_results.json\n",
            "An error occurred during analysis execution: object dict can't be used in 'await' expression\n"
          ]
        }
      ]
    }
  ]
}